{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huynguyeny2/Effects-of-Advertizing-on-Sales/blob/main/SVHN_Digit_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q91KqmCRu64D"
      },
      "source": [
        "# **Deep Learning Project: Street View Housing Number Digit Recognition**\n",
        "\n",
        "# **Marks: 60**\n",
        "\n",
        "\n",
        "--------------\n",
        "## **Context**\n",
        "--------------\n",
        "\n",
        "One of the most interesting tasks in deep learning is to recognize objects in natural scenes. The ability to process visual information using machine learning algorithms can be very useful as demonstrated in various applications.\n",
        "\n",
        "The SVHN dataset contains over 600,000 labeled digits cropped from street-level photos. It is one of the most popular image recognition datasets. It has been used in neural networks created by Google to improve the map quality by automatically transcribing the address numbers from a patch of pixels. The transcribed number with a known street address helps pinpoint the location of the building it represents.\n",
        "\n",
        "----------------\n",
        "## **Objective**\n",
        "----------------\n",
        "\n",
        "Our objective is to predict the number depicted inside the image by using Artificial or Fully Connected Feed Forward Neural Networks and Convolutional Neural Networks. We will go through various models of each and finally select the one that is giving us the best performance.\n",
        "\n",
        "-------------\n",
        "## **Dataset**\n",
        "-------------\n",
        "Here, we will use a subset of the original data to save some computation time. The dataset is provided as a .h5 file. The basic preprocessing steps have been applied on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ef2b3c"
      },
      "source": [
        "### **Please read the instructions carefully before starting the project.**\n",
        "\n",
        "This is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned. Read along carefully to complete the project.\n",
        "\n",
        "* Blanks '_______' are provided in the notebook that needs to be filled with an appropriate code to get the correct result. Please replace the blank with the right code snippet. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space.\n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Fill the code wherever asked by the commented lines like \"# Fill in the blank\" or \"# Complete the code\". Running incomplete code may throw an error.\n",
        "* Remove the blank and state your observations in detail wherever the mark down says 'Write your observations here:_________'\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* You can the results/observations derived from the analysis here and use them to create your final report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z2Z7-OAs8QG"
      },
      "source": [
        "## **Mount the drive**\n",
        "\n",
        "Let us start by mounting the Google drive. You can run the below cell to mount the Google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "03lDyQUuef7z",
        "outputId": "c74b58a4-62ab-491b-904e-803961faa433"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-caacbe6b-cca5-4eb2-8cf1-ba8b7979b137\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-caacbe6b-cca5-4eb2-8cf1-ba8b7979b137\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8U3DUa3eNsT"
      },
      "source": [
        "## **Importing the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dVzeuF3eQx1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, BatchNormalization, LeakyReLU\n",
        "\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucnevGLoyKf_"
      },
      "source": [
        "**Let us check the version of tensorflow.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5as47YxyJVk"
      },
      "outputs": [],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lsux2ZwyTTR"
      },
      "source": [
        "## **Load the dataset**\n",
        "\n",
        "- Let us now load the dataset that is available as a .h5 file.\n",
        "- Split the data into the train and the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BApX9qgNsqV0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "# Open the file as read only\n",
        "# User can make changes in the path as required\n",
        "\n",
        "h5f = h5py.File('SVHN_single_grey1.h5', 'r')\n",
        "\n",
        "# Load the training and the test dataset\n",
        "\n",
        "X_train = h5f['X_train'][:]\n",
        "\n",
        "y_train = h5f['y_train'][:]\n",
        "\n",
        "X_test = h5f['X_test'][:]\n",
        "\n",
        "y_test = h5f['y_test'][:]\n",
        "\n",
        "\n",
        "# Close this file\n",
        "\n",
        "h5f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVe0CYpUgj7w"
      },
      "source": [
        "Let's check the number of images in the training and the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3lwKpOefkpA"
      },
      "outputs": [],
      "source": [
        "len(X_train), len(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akTUOfLlgwoM"
      },
      "source": [
        "**Observation:**\n",
        "- There are 42,000 images in the training data and 18,000 images in the testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxODV6HKykuc"
      },
      "source": [
        "## **Visualizing images**\n",
        "\n",
        "- Use X_train to visualize the first 10 images.\n",
        "- Use Y_train to print the first 10 labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvsc8ytHsqWD"
      },
      "outputs": [],
      "source": [
        "# Visualizing the first 10 images in the dataset and printing their labels\n",
        "\n",
        "plt.figure(figsize = (10, 1))\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "    plt.subplot(1, 10, i+1)\n",
        "\n",
        "    plt.imshow(X_train[i], cmap = \"gray\")\n",
        "\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print('label for each of the above image: %s' % (y_train[0:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzoyeXHOy80N"
      },
      "source": [
        "## **Data preparation**\n",
        "\n",
        "- Print the shape and the array of pixels for the first image in the training dataset.\n",
        "- Normalize the train and the test dataset by dividing by 255.\n",
        "- Print the new shapes of the train and the test dataset.\n",
        "- One-hot encode the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqndzQXng9rL"
      },
      "outputs": [],
      "source": [
        "# Shape and the array of pixels for the first image\n",
        "\n",
        "print(\"Shape:\", X_train[0].shape)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"First image:\\n\", X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9YPwf9ysqWU"
      },
      "outputs": [],
      "source": [
        "# Reshaping the dataset to flatten them. We are reshaping the 2D image into 1D array\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 1024)\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], 1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4CQkKtQ0XII"
      },
      "source": [
        "### **Normalize the train and the test data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_yUUTp_mUzB"
      },
      "outputs": [],
      "source": [
        "# Normalize inputs from 0-255 to 0-1\n",
        "\n",
        "X_train = X_train.astype('float32')/255.0\n",
        "\n",
        "X_test = X_test.astype('float32')/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7FSqOpamWkH"
      },
      "outputs": [],
      "source": [
        "# New shape\n",
        "\n",
        "print('Training set:', X_train.shape, y_train.shape)\n",
        "\n",
        "print('Test set:', X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL0lYER4sqWw"
      },
      "outputs": [],
      "source": [
        "# One-hot encode output\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Test labels\n",
        "\n",
        "y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViqPOTquCF76"
      },
      "source": [
        "**Observation:**\n",
        "- Notice that each entry of the target variable is a one-hot encoded vector instead of a single label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH-gVrzuByNA"
      },
      "source": [
        "## **Model Building**\n",
        "\n",
        "Now that we have done the data preprocessing, let's build an ANN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcKRwrGn0XIL"
      },
      "outputs": [],
      "source": [
        "# Fixing the seed for random number generators\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJDUoaEj1d6e"
      },
      "source": [
        "### **Model Architecture**\n",
        "- Write a function that returns a sequential model with the following architecture:\n",
        " - First hidden layer with **64 nodes and the relu activation** and the **input shape = (1024, )**\n",
        " - Second hidden layer with **32 nodes and the relu activation**\n",
        " - Output layer with **activation as 'softmax' and number of nodes equal to the number of classes, i.e., 10**\n",
        " - Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n",
        "- Call the nn_model_1 function and store the model in a new variable.\n",
        "- Print the summary of the model.\n",
        "- Fit on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 20**. Store the model building history to use later for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A48z6ucF0XIP"
      },
      "source": [
        "### **Build and train an ANN model as per the above mentioned architecture.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmi81Gr5sqW-"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "\n",
        "def nn_model_1():\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add layers as per the architecture mentioned above in the same sequence\n",
        "\n",
        "    model.add(Dense(64, activation='relu', input_shape=(1024,)))\n",
        "\n",
        "    model.add(Dense(32, activation = 'relu'))\n",
        "\n",
        "    model.add(Dense(10, activation = 'softmax'))\n",
        "    opt = Adam(learning_rate=0.001)\n",
        "\n",
        "    # Compile the model\n",
        "\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGCUI_xsImnH"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "\n",
        "model_1 = nn_model_1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckJsLdmdQadZ"
      },
      "outputs": [],
      "source": [
        "# Print the summary\n",
        "\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fy3Dif_zcCk"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "\n",
        "history_model_1 = model_1.fit(X_train, y_train, validation_split=0.2, batch_size=128, verbose=1, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKOckG-KPyLg"
      },
      "source": [
        "### **Plotting the validation and training accuracies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeF8XSWz0XIU"
      },
      "source": [
        "### **Write your observations on the below plot.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt77zgGMP4yw"
      },
      "outputs": [],
      "source": [
        "# Plotting the accuracies\n",
        "\n",
        "dict_hist = history_model_1.history\n",
        "\n",
        "list_ep = [i for i in range(1, 21)]\n",
        "\n",
        "plt.figure(figsize = (8, 8))\n",
        "\n",
        "plt.plot(list_ep, dict_hist['accuracy'], ls = '--', label = 'accuracy')\n",
        "\n",
        "plt.plot(list_ep, dict_hist['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGBbQpLONX7k"
      },
      "source": [
        "**Observations:The model exhbits a good estimate over the actual data. The predicted model is able to generalized with very little noise. This indicate a good approximate for the accuracy between the testing model to the training model. The model shown has a validation accuracy of 68%. The data is consistent even after 20 epochs.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0qgLMBZm5-K"
      },
      "source": [
        "Let's build one more model with higher complexity and see if we can improve the performance of the model.\n",
        "\n",
        "First, we need to clear the previous model's history from the Keras backend. Also, let's fix the seed again after clearing the backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y98BpLxsok_H"
      },
      "outputs": [],
      "source": [
        "# Clearing backend\n",
        "\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbKi93HTolGW"
      },
      "outputs": [],
      "source": [
        "# Fixing the seed for random number generators\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT6o3TIKuCtk"
      },
      "source": [
        "### **Second Model Architecture**\n",
        "- Write a function that returns a sequential model with the following architecture:\n",
        " - First hidden layer with **256 nodes and the relu activation** and the **input shape = (1024, )**\n",
        " - Second hidden layer with **128 nodes and the relu activation**\n",
        " - Add the **Dropout layer with the rate equal to 0.2**\n",
        " - Third hidden layer with **64 nodes and the relu activation**\n",
        " - Fourth hidden layer with **64 nodes and the relu activation**\n",
        " - Fifth hidden layer with **32 nodes and the relu activation**\n",
        " - Add the **BatchNormalization layer**\n",
        " - Output layer with **activation as 'softmax' and number of nodes equal to the number of classes, i.e., 10**\n",
        " -Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.0005), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n",
        "- Call the nn_model_2 function and store the model in a new variable.\n",
        "- Print the summary of the model.\n",
        "- Fit on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 30**. Store the model building history to use later for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-ZjNBmH0XIV"
      },
      "source": [
        "### **Build and train the new ANN model as per the above mentioned architecture.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEPYLFIPnSDP"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "\n",
        "def nn_model_2():\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add layers as per the architecture mentioned above in the same sequence\n",
        "\n",
        "    model.add(Dense(256, activation='relu', input_shape=(1024,)))\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    opt = Adam(learning_rate=0.0005)\n",
        "\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKjDY4plnSFl"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "\n",
        "model_2 = nn_model_2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz_6SVrU2swZ"
      },
      "outputs": [],
      "source": [
        "# Print the model summary\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ0Sc4R-y-uu"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "\n",
        "history_model_2 = model_2.fit(X_train, y_train, validation_split=0.2, batch_size=128, verbose=1, epochs=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UjEIbPM0XIX"
      },
      "source": [
        "### **Plotting the validation and training accuracies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJYsvjmw0XIX"
      },
      "source": [
        "### **Write your observations on the below plot.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01ig6BrF1KVy"
      },
      "outputs": [],
      "source": [
        "# Plotting the accuracies\n",
        "\n",
        "dict_hist = history_model_2.history\n",
        "\n",
        "list_ep = [i for i in range(1, 31)]\n",
        "\n",
        "plt.figure(figsize = (8, 8))\n",
        "\n",
        "plt.plot(list_ep, dict_hist['accuracy'], ls = '--', label = 'accuracy')\n",
        "\n",
        "plt.plot(list_ep, dict_hist['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPW1LlD61RDn"
      },
      "source": [
        "**Observations:The model becomes a little shaky after 12 epochs and has a validation accuracy of 73%.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kuXx9Bvu00f"
      },
      "source": [
        "## **Predictions on the test data**\n",
        "\n",
        "- Make predictions on the test set using the second model.\n",
        "- Print the obtained results using the classification report and the confusion matrix.\n",
        "- Final observations on the obtained results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbWMEtTj5Ad0"
      },
      "outputs": [],
      "source": [
        "test_pred = model_2.predict(X_test)\n",
        "\n",
        "test_pred = np.argmax(test_pred, axis = -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3li8Ib08yts"
      },
      "source": [
        "**Note:** Earlier, we noticed that each entry of the target variable is a one-hot encoded vector but to print the classification report and confusion matrix, we must convert each entry of y_test to a single label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NByu7uAQ8x9P"
      },
      "outputs": [],
      "source": [
        "# Converting each entry to single label from one-hot encoded vector\n",
        "\n",
        "y_test = np.argmax(y_test, axis = -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_SIoopr0XIg"
      },
      "source": [
        "### **Print the classification report and the confusion matrix for the test predictions. Write your observations on the final results.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRddeJ-3EHT1"
      },
      "outputs": [],
      "source": [
        "# Importing required functions\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Printing the classification report\n",
        "\n",
        "print(classification_report(y_test, test_pred))\n",
        "\n",
        "# Plotting the heatmap using confusion matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, test_pred)     #Write the code for creating confusion matrix using actual labels (y_test) and predicted labels (test_pred)\n",
        "\n",
        "plt.figure(figsize = (8, 5))\n",
        "\n",
        "sns.heatmap(cm, annot = True,  fmt = '.0f')\n",
        "\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "plt.xlabel('Predicted')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjErl4GA2u9s"
      },
      "source": [
        "**Final Observations:__________**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkR4JioMsuIV"
      },
      "source": [
        "## **Using Convolutional Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jDqh2niv7u2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "h5f = h5py.File('SVHN_single_grey1.h5', 'r')\n",
        "\n",
        "# Load the training and the test dataset\n",
        "\n",
        "X_train = h5f['X_train'][:]\n",
        "\n",
        "y_train = h5f['y_train'][:]\n",
        "\n",
        "X_test = h5f['X_test'][:]\n",
        "\n",
        "y_test = h5f['y_test'][:]\n",
        "\n",
        "\n",
        "# Close this file\n",
        "\n",
        "h5f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCX90mBX7CyN"
      },
      "source": [
        "Let's check the number of images in the training and the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB_CPqX97CyN"
      },
      "outputs": [],
      "source": [
        "len(X_train), len(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-2ZS88A7CyV"
      },
      "source": [
        "**Observation:**\n",
        "- There are 42,000 images in the training data and 18,000 images in the testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6wJj56u7CyW"
      },
      "source": [
        "## **Data preparation**\n",
        "\n",
        "- Print the shape and the array of pixels for the first image in the training dataset.\n",
        "- Reshape the train and the test dataset because we always have to give a 4D array as input to CNNs.\n",
        "- Normalize the train and the test dataset by dividing by 255.\n",
        "- Print the new shapes of the train and the test dataset.\n",
        "- One-hot encode the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFbbhntr7CyW"
      },
      "outputs": [],
      "source": [
        "# Shape and the array of pixels for the first image\n",
        "\n",
        "print(\"Shape:\", X_train[0].shape)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"First image:\\n\", X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZqaO-nU7CyW"
      },
      "outputs": [],
      "source": [
        "# Reshaping the dataset to be able to pass them to CNNs. Remember that we always have to give a 4D array as input to CNNs\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 32, 32, 1)\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], 32, 32, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4I6Fq_h7CyW"
      },
      "outputs": [],
      "source": [
        "# Normalize inputs from 0-255 to 0-1\n",
        "\n",
        "X_train = X_train / 255.0\n",
        "\n",
        "X_test = X_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSIgVj6I7CyW"
      },
      "outputs": [],
      "source": [
        "# New shape\n",
        "\n",
        "print('Training set:', X_train.shape, y_train.shape)\n",
        "\n",
        "print('Test set:', X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10QaOV-xR7Jn"
      },
      "source": [
        "### **One-hot encode the labels in the target variable y_train and y_test.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9-cx9yS7CyW"
      },
      "outputs": [],
      "source": [
        "# Write the function and appropriate variable name to one-hot encode the output\n",
        "\n",
        "# One-hot encode output\n",
        "\n",
        "y_train_encoded = tf.keras.utils.to_categorical(y_train)\n",
        "\n",
        "y_test_encoded = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Test labels\n",
        "\n",
        "y_test_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXVwLJ2-7CyW"
      },
      "source": [
        "**Observation:**\n",
        "- Notice that each entry of the target variable is a one-hot encoded vector instead of a single label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3VVYhZd7CyW"
      },
      "source": [
        "## **Model Building**\n",
        "\n",
        "Now that we have done data preprocessing, let's build a CNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY5pyF4-KDNt"
      },
      "outputs": [],
      "source": [
        "# Fixing the seed for random number generators\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkZpgyzw7CyX"
      },
      "source": [
        "### **Model Architecture**\n",
        "- **Write a function** that returns a sequential model with the following architecture:\n",
        " - First Convolutional layer with **16 filters and the kernel size of 3x3**. Use the **'same' padding** and provide the **input shape = (32, 32, 1)**\n",
        " - Add a **LeakyRelu layer** with the **slope equal to 0.1**\n",
        " - Second Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**\n",
        " - Another **LeakyRelu** with the **slope equal to 0.1**\n",
        " - A **max-pooling layer** with a **pool size of 2x2**\n",
        " - **Flatten** the output from the previous layer\n",
        " - Add a **dense layer with 32 nodes**\n",
        " - Add a **LeakyRelu layer with the slope equal to 0.1**\n",
        " - Add the final **output layer with nodes equal to the number of classes, i.e., 10** and **'softmax' as the activation function**\n",
        " - Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n",
        "- Call the function cnn_model_1 and store the output in a new variable.\n",
        "- Print the summary of the model.\n",
        "- Fit the model on the training data with a **validation split of 0.2, batch size = 32, verbose = 1, and epochs = 20**. Store the model building history to use later for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWsAd45JKDNu"
      },
      "source": [
        "### **Build and train a CNN model as per the above mentioned architecture.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyPeYlqS7CyX"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "\n",
        "def cnn_model_1():\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add layers as per the architecture mentioned above in the same sequence\n",
        "    # First Convolutional layer\n",
        "    model.add(Conv2D(16, (3, 3), padding='same', input_shape=(32, 32, 1)))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "    # Second Convolutional layer\n",
        "    model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "    # Max-pooling layer\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Dense layer\n",
        "    model.add(Dense(32))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    opt = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUyktN6F7CyX"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "\n",
        "cnn_model_1 = cnn_model_1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mAjeB5z7CyX"
      },
      "outputs": [],
      "source": [
        "# Print the model summary\n",
        "\n",
        "cnn_model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ioer0DV7CyX"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "\n",
        "history_cnn_model_1 = cnn_model_1.fit(\n",
        "    X_train, y_train_encoded,\n",
        "    validation_split=0.2,\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfiAHXdS7CyX"
      },
      "source": [
        "### **Plotting the validation and training accuracies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPzfIf9kKDNw"
      },
      "source": [
        "### **Write your observations on the below plot.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDt-OYSh7CyX"
      },
      "outputs": [],
      "source": [
        "# Plotting the accuracies\n",
        "\n",
        "dict_hist = history_cnn_model_1.history\n",
        "\n",
        "list_ep = [i for i in range(1, 21)]\n",
        "\n",
        "plt.figure(figsize = (8, 8))\n",
        "\n",
        "plt.plot(list_ep, dict_hist['accuracy'], ls = '--', label = 'accuracy')\n",
        "\n",
        "plt.plot(list_ep, dict_hist['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A1gOsqS7CyX"
      },
      "source": [
        "**Observations:We can see from the above plot that the model has perfomed well on train and validation data with a validation accuracy of 87%.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2h5TcDt7CyX"
      },
      "source": [
        "Let's build another model and see if we can get a better model with generalized performance.\n",
        "\n",
        "First, we need to clear the previous model's history from the Keras backend. Also, let's fix the seed again after clearing the backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2XohRDC7CyX"
      },
      "outputs": [],
      "source": [
        "# Clearing backend\n",
        "\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOHjBjeP7CyY"
      },
      "outputs": [],
      "source": [
        "# Fixing the seed for random number generators\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwRS3BbI7CyY"
      },
      "source": [
        "### **Second Model Architecture**\n",
        "\n",
        "- Write a function that returns a sequential model with the following architecture:\n",
        " - First Convolutional layer with **16 filters and the kernel size of 3x3**. Use the **'same' padding** and provide the **input shape = (32, 32, 1)**\n",
        " - Add a **LeakyRelu layer** with the **slope equal to 0.1**\n",
        " - Second Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**\n",
        " - Add **LeakyRelu** with the **slope equal to 0.1**\n",
        " - Add a **max-pooling layer** with a **pool size of 2x2**\n",
        " - Add a **BatchNormalization layer**\n",
        " - Third Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**\n",
        " - Add a **LeakyRelu layer with the slope equal to 0.1**\n",
        " - Fourth Convolutional layer **64 filters and the kernel size of 3x3 with 'same' padding**\n",
        " - Add a **LeakyRelu layer with the slope equal to 0.1**\n",
        " - Add a **max-pooling layer** with a **pool size of 2x2**\n",
        " - Add a **BatchNormalization layer**\n",
        " - **Flatten** the output from the previous layer\n",
        " - Add a **dense layer with 32 nodes**\n",
        " - Add a **LeakyRelu layer with the slope equal to 0.1**\n",
        " - Add a **dropout layer with the rate equal to 0.5**\n",
        " - Add the final **output layer with nodes equal to the number of classes, i.e., 10** and **'softmax' as the activation function**\n",
        " - Compile the model with the **categorical_crossentropy loss, adam optimizers (learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n",
        "- Call the function cnn_model_2 and store the model in a new variable.\n",
        "- Print the summary of the model.\n",
        "- Fit the model on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 30**. Store the model building history to use later for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5IBLS1eKDNy"
      },
      "source": [
        "### **Build and train the second CNN model as per the above mentioned architecture.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz3O8ML27CyY"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "\n",
        "def cnn_model_2():\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add layers as per the architecture mentioned above in the same sequence\n",
        "    # First Convolutional layer\n",
        "    model.add(Conv2D(16, (3, 3), padding='same', input_shape=(32, 32, 1)))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "    # Second Convolutional layer\n",
        "    model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "    # Max-pooling layer\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # BatchNormalization layer\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Third Convolutional layer\n",
        "    model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "    # Fourth Convolutional layer\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "    # Max-pooling layer\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # BatchNormalization layer\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Dense layer\n",
        "    model.add(Dense(32))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "    # Dropout layer\n",
        "    model.add(Dropout(rate=0.5))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    opt = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soQ_mNW37CyY"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "\n",
        "cnn_model_2 = cnn_model_2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ3hqzlg7CyY"
      },
      "outputs": [],
      "source": [
        "# Print the summary\n",
        "\n",
        "cnn_model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRyPGCxe7CyY"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "\n",
        "history_cnn_model_2 = cnn_model_2.fit(\n",
        "    X_train, y_train_encoded,\n",
        "    validation_split=0.2,\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKCdy59GKDN1"
      },
      "source": [
        "### **Plotting the validation and training accuracies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyhUtMy3KDN1"
      },
      "source": [
        "### **Write your observations on the below plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0JBpqI-7CyY"
      },
      "outputs": [],
      "source": [
        "# Plotting the accuracies\n",
        "\n",
        "dict_hist = history_cnn_model_2.history\n",
        "\n",
        "list_ep = [i for i in range(1, 31)]\n",
        "\n",
        "plt.figure(figsize = (8, 8))\n",
        "\n",
        "plt.plot(list_ep, dict_hist['accuracy'], ls = '--', label = 'accuracy')\n",
        "\n",
        "plt.plot(list_ep, dict_hist['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqJXvAi87CyY"
      },
      "source": [
        "**Observations:________**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrh7kQyT7CyY"
      },
      "source": [
        "## **Predictions on the test data**\n",
        "\n",
        "- Make predictions on the test set using the second model.\n",
        "- Print the obtained results using the classification report and the confusion matrix.\n",
        "- Final observations on the obtained results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHCRwRbgKDN2"
      },
      "source": [
        "### **Make predictions on the test data using the second model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSnc1vQf7CyY"
      },
      "outputs": [],
      "source": [
        "# Make prediction on the test data using model_2\n",
        "\n",
        "test_pred = cnn_model_2.predict(X_train)\n",
        "\n",
        "test_pred = np.argmax(test_pred, axis = -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLFpL_LS7CyY"
      },
      "source": [
        "**Note:** Earlier, we noticed that each entry of the target variable is a one-hot encoded vector, but to print the classification report and confusion matrix, we must convert each entry of y_test to a single label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76mmdglM7CyY"
      },
      "outputs": [],
      "source": [
        "# Converting each entry to single label from one-hot encoded vector\n",
        "\n",
        "y_test = np.argmax(y_test, axis = -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVCa-ysWKDN3"
      },
      "source": [
        "### **Write your final observations on the performance of the model on the test data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVfBgpNa7CyZ"
      },
      "outputs": [],
      "source": [
        "# Importing required functions\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Printing the classification report\n",
        "\n",
        "print(classification_report(y_test, test_pred))\n",
        "\n",
        "# Plotting the heatmap using confusion matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "\n",
        "plt.figure(figsize = (8, 5))\n",
        "\n",
        "sns.heatmap(cm, annot = True,  fmt = '.0f')\n",
        "\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "plt.xlabel('Predicted')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-v8X_LO7Cya"
      },
      "source": [
        "**Final Observations:We observe that most of the classes are predicted correctly. With an F1 score of 92%**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}